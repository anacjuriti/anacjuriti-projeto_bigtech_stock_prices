# -*- coding: utf-8 -*-
"""notebook_DataLab - Stock prices.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ngBmEmy8KHC8pAFhwfKvcQIwYCjoKW0Y

###Bibliotecas
"""

import pandas as pd #corr() correlação #importa a biblioteca Pandas para manipulação de dados
import numpy as np
import matplotlib.pyplot as plt #importa a biblioteca Matplotlib para criar visualizações
import seaborn as sns
#importlib_metadata
import missingno as msno

"""###Importação do DataFrame Atualizado"""

from google.colab import drive
drive.mount('/content/drive')

# Carregar dados de um arquivo CSV
df = pd.read_csv('/content/stock_prices_clean.csv')

"""# Preparação da base de dados

Conectar/importar dados
"""

import pandas as pd
import numpy as np

# Importar os dados
# Carregar dados de um arquivo CSV
df = pd.read_excel('/content/big_tech_stock_prices.xlsx')

# Verificar o tipo de dados de cada coluna
print(df.dtypes)

# Alterar tipos de dados se necessário
#df['volume'] = df['volume'].astype(int)
#df['date'] = pd.to_datetime(df['date'])

# Visualizar as primeiras linhas do dataframe
print(df.head())

""" Identificar e Gerenciar Valores Nulos"""

#valores nulos
print(df.isnull().sum())

#Remover linhas com valores nulos
#df.dropna(inplace=True)

"""Identificar e Gerenciar Valores Duplicados"""

# Verificar duplicados
print(df.duplicated().sum())

# Contar o número de linhas duplicadas
num_duplicados = df.duplicated().sum()
print(f"Número de linhas duplicadas: {num_duplicados}")

# Visualizar as linhas duplicadas
duplicados = df[df.duplicated()]
print("Linhas duplicadas:")
print(duplicados)

# Visualizar as primeiras ocorrências e suas duplicatas
duplicados_detalhados = df[df.duplicated(keep=False)]
print("Primeiras ocorrências e suas duplicatas:")
print(duplicados_detalhados) #print(duplicados_detalhados.head())

# Remover todas as linhas duplicadas (mantendo apenas a primeira ocorrência)
df_sem_duplicados = df.drop_duplicates() #contém todas as linhas únicas do DataFrame original df

# Verificar se as linhas duplicadas foram removidas
print(df_sem_duplicados.duplicated().sum())
print(df_sem_duplicados.head())

#Atualizar o DataFrame original
df = df.drop_duplicates()
print("DataFrame após remover duplicatas (mantendo a primeira ocorrência):")
print(df.head())

# Verificar o DataFrame após remover duplicatas
print(f"Forma após remover duplicatas: {df_sem_duplicados.shape}") #dataframe com duplicada removida
print(f"Forma após remover duplicatas: {df.shape}") #dataframe original, contendo duplicada

#renomear dataframe com nome do dataframe sem duplicatas
#df_sem_duplicados.name = df.name

"""Unir Tabelas"""

#unir a tabela Bigtechstock_prices com a tabela Bigtechcompanies

#importar df companies
df_companies = pd.read_csv('/content/big_tech_companies.csv')

#join
df = pd.merge(df, df_companies, on='stock_symbol', how='inner')
print(df.head())

"""Criar Novas Variáveis"""

#nova variável 'prince_range' = diferença entre 'high' e 'low' (prince =! price)
df['prince_range'] = df['high'] - df['low']
print(df.head())

#nova variável 'daily_return' = variação percentual de adj_close (preço de fechamento ajustado)
df['daily_return'] = df['adj_close'].pct_change()
print(df.head())

# Lista com a nova ordem das colunas
nova_ordem = ['stock_symbol', 'company', 'date', 'open', 'high', 'low', 'close', 'adj_close', 'volume']

# Reordenar as colunas
#df_merged = df_merged[nova_ordem]

# Verificar o resultado
#print(df_merged.head())

#formato do dataframe
forma_df = df.shape
print(f"Nº de linhas: {forma_df[0]}")
print(f"Nº de colunas: {forma_df[1]}")

print(df.dtypes)
print(df.head())

#número de linhas e colunas
print(f"Número de linhas: {forma_df[0]}")
print(f"Número de colunas: {forma_df[1]}")

# Lista com a nova ordem das colunas
nova_ordem = ['stock_symbol', 'company', 'date', 'open', 'high', 'low', 'price_range', 'close', 'adj_close', 'daily_return' , 'volume']

# Reordenar as colunas
df = df[nova_ordem]

# Verificar o resultado
print(df.head())

#renomear prince_range para price_range
df['price_range'] = df['prince_range']
df.drop('prince_range', axis=1, inplace=True)

"""Construir Tabelas Auxiliares"""

# Estatísticas descritivas para cada 'stock_symbol'
tabela_auxiliar = df.groupby('stock_symbol').describe()
print(tabela_auxiliar)

"""Identificar e Gerenciar Dados Discrepantes em Variáveis Numéricas"""

#identificar outliers  na coluna 'close' com método do desvio padrão
mean = df['close'].mean()
std = df['close'].std()
outliers = df[(df['close'] < (mean - 3 * std)) | (df['close'] > (mean + 3 * std))]
print(outliers)

# Remover outliers
#df = df[(df['close'] >= (mean - 3 * std)) & (df['close'] <= (mean + 3 * std))]
#print(df)

"""Identificar e Gerir Dados Discrepantes em Variáveis Categóricas"""

# Verificar valores únicos na coluna 'stock_symbol'
print(df_merged['stock_symbol'].unique())

"""Identificar e Manejar Dados Fora Do Alcance Da Análise"""

#garantir que a coluna 'volume' não tenha valores não negativos
df_merged = df_merged[df_merged['volume'] >= 0]

print(df.info())

"""Salvar o DataFrame atualizado"""

#salvar novo DataFrame em um arquivo CSV
df.to_csv('/content/stock_prices_clean.csv', index=False)

df = pd.read_csv('/content/stock_prices_clean.csv')
df = pd.read_excel('/content/big_tech_stock_prices.xlsx')

print(df.head())
print(df.shape)
print(df.info())
print(df.describe())

"""# Análise exploratória

## Dados gerais
"""

df = pd.read_csv ('/content/stock_prices_clean.csv')

print(df.info())

#linhas duplicadas
num_duplicados = df.duplicated().sum()
print(f"Número de linhas duplicadas: {num_duplicados}")

#linhas/colunas
df.shape
print(f"Número de linhas: {df.shape[0]}")
print(f"Número de colunas: {df.shape[1]}")

#matriz de nulos
msno.matrix(df, figsize=(8,4))

#gráfico de nulos
msno.bar(df, figsize=(8,4))

"""## Desempenho geral de cada empresa

###Análise Descritiva
"""

#Estatísticas descritivas
#média, mediana, desvio padrão, mínimo e máximo
#dos preços de abertura, alto, baixo, diferença alto/baixo, fechamento, fechamento ajustado, variação close/adj_close e volume de negociações
df.describe()

"""###Medidas de Tendência Central"""

#média dos preços de abertura para cada empresa
mean_open = df.groupby('company')['open'].mean() #agrupa os dados por empresa, seleciona a coluna de preços de abertura e calcula a média de 'open'
print("Média dos Preços de Abertura para Cada Empresa:")
print(mean_open)

#gráfico
mean_open.plot(kind='bar', title='Média dos Preços de Abertura por Empresa', figsize=(8, 6))
plt.xlabel('Empresa')
plt.ylabel('Preço de Abertura Médio')
plt.show()

#mediana dos preços de fechamento para cada empresa
median_close = df.groupby('company')['close'].median() #agrupa os dados por empresa, seleciona a coluna de preços de fechamento e calcula a mediana de 'close'
print("\nMediana dos Preços de Fechamento para Cada Empresa:")
print(median_close)

#gráfico
median_close.plot(kind='bar', title='Mediana dos Preços de Fechamento por Empresa', figsize=(8, 6))
plt.xlabel('Empresa')
plt.ylabel('Preço de Fechamento Mediano')
plt.show()

#moda dos volumes de ações para cada empresa
mode_volume = df.groupby('company')['volume'].agg(lambda x: x.mode().iloc[0] if not x.mode().empty else None)
#agrupa os dados por empresa, seleciona a coluna de volume e calcula a moda de 'volume'
print("\nModa dos Volumes de Negociação para Cada Empresa:")
print(mode_volume)

#gráfico
mode_volume.plot(kind='bar', title='Moda dos Volumes de Negociação por Empresa', figsize=(8, 6))
plt.xlabel('Empresa')
plt.ylabel('Volume de Negociação Moda')
plt.show()

"""###Medidas de Dispersão"""

#variância dos preços de abertura para cada empresa
variance_open = df.groupby('company')['open'].var() #agrupa os dados por empresa, seleciona a coluna de preços de abertura e calcula  a variância  de 'open'
print("Variância dos Preços de Abertura para Cada Empresa:")
print(variance_open)

#desvio padrão dos preços de fechamento para cada empresa
std_close = df.groupby('company')['close'].std() #agrupa os dados por empresa, seleciona a coluna de preços de fechamento e calcula o desvio padrão de 'close'
print("\nDesvio Padrão dos Preços de Fechamento para Cada Empresa:")
print(std_close)

#gráfico
std_close.plot(kind='bar', title='Desvio Padrão dos Preços de Fechamento por Empresa', figsize=(8, 6))
plt.xlabel('Empresa')
plt.ylabel('Desvio Padrão')
plt.show()

#amplitude (range) dos preços de fechamento para cada empresa
range_close = df.groupby('company').apply(lambda x: x['close'].max() - x['close'].min())
#agrupa os dados por empresa e calcula  a amplitude de 'close'
print("\nAmplitude dos Preços de Fechamento para Cada Empresa:")
print(range_close)

#gráfico
range_close.plot(kind='bar', title='Amplitude dos Preços de Fechamento por Empresa', figsize=(8, 6))
plt.xlabel('Empresa')
plt.ylabel('Amplitude')
plt.show()

"""###Quartis"""

#quartis dos preços ajustados de fechamento para cada empresa
quartiles_adj_close = df.groupby('company')['adj_close'].quantile([0.25, 0.5, 0.75, 1]).unstack()
#agrupa os dados por empresa, seleciona a coluna de preços ajustados de fechamento e calcula os quartis (25%, 50%, 75%, 100%) de 'adj_close'
print("\nQuartis dos Preços Ajustados de Fechamento para Cada Empresa:")
print(quartiles_adj_close)

#gráfico
quartiles_adj_close.plot(kind='bar', title='Quartis dos Preços Ajustados de Fechamento por Empresa', figsize=(8, 6))
plt.xlabel('Empresa')
plt.ylabel('Preço Ajustado de Fechamento')
plt.show()

"""##Comportamento dos dados de ações ao longo do tempo

###Análise Temporal
"""

#date definida como o índice
df.set_index('date', inplace=True)

"""Tendência dos preços ajustados de fechamento (adj_close)"""

#14 empresas
#tendência dos preços ajustados de fechamento (adj_close)
companies = df['company'].unique()

#preço no fechamento do mercado ao longo do tempo para cada empresa
plt.figure(figsize=(12, 6))

for company in companies:
    company_data = df[df['company'] == company]
    plt.plot(company_data.index, company_data['adj_close'], label=company)

plt.title('Preço de Fechamento ao Longo do Tempo')
plt.xlabel('Data')
plt.ylabel('Preço Ajustado de Fechamento (adj_close)')
plt.legend()
plt.show()

"""Variação do preço de abertura (open) e fechamento (close) do mercado"""

#preço de abertura e fechamento ao longo do tempo para cada empresa
plt.figure(figsize=(12, 8))

for company in companies:
    company_data = df[df['company'] == company]
    plt.plot(company_data.index, company_data['open'], label=f'{company} - Open', linestyle='--')
    plt.plot(company_data.index, company_data['close'], label=f'{company} - Close')

plt.title('Preços de Abertura e Fechamento ao Longo do Tempo')
plt.xlabel('Data')
plt.ylabel('Preços (open e close)')
plt.legend()
plt.show()

"""gráfico de dispersão dos preços de abertura vs fechamento"""

plt.figure(figsize=(12, 8))

for company in companies:
    company_data = df[df['company'] == company]
    plt.scatter(company_data['open'], company_data['close'], label=company, alpha=0.5)

plt.title('Comparação dos Preços de Abertura vs Fechamento')
plt.xlabel('Preço de Abertura (open)')
plt.ylabel('Preço de Fechamento (close)')
plt.legend()
plt.show()

"""Volume de ações ao longo do tempo"""

plt.figure(figsize=(10, 8))

for company in companies:
    company_data = df[df['company'] == company]
    plt.plot(company_data.index, company_data['volume'], label=company)

plt.title('Volume de Ações ao Longo do Tempo')
plt.xlabel('Data')
plt.ylabel('Volume')
plt.legend()
plt.show()

"""Retornos diários ao longo do tempo"""

plt.figure(figsize=(12, 10))

for company in companies:
    company_data = df[df['company'] == company]
    plt.plot(company_data.index, company_data['daily_return'], label=company)

plt.title('Retornos Diários ao Longo do Tempo')
plt.xlabel('Data')
plt.ylabel('Retorno Diário')
plt.legend()
plt.show()

"""Médias móveis para identificar tendências

As médias móveis são úteis para suavizar as séries temporais e identificar tendências de longo prazo.
"""

#cálculo de médias móveis simples de 20 dias

#tamanho da janela para a média móvel simples
window_size = 20

plt.figure(figsize=(12, 10))

#loop através de cada empresa para calcular e plotar a média móvel simples
for company in companies:
    company_data = df[df['company'] == company].copy()  #usar .copy() para evitar o SettingWithCopyWarning (permite fazer alterações sem afetar o dataframe original)
    company_data['SMA'] = company_data['adj_close'].rolling(window=window_size).mean() #calcula a média móvel simples para adj_close
    plt.plot(company_data.index, company_data['adj_close'], label=f'{company} - Preço Ajustado')
    plt.plot(company_data.index, company_data['SMA'], label=f'{company} - {window_size}-Day SMA') #plota a média móvel simples calculada

#títulos e rótulos do gráfico
plt.title(f'Médias Móveis Simples de {window_size} Dias ao Longo do Tempo')
plt.xlabel('Data')
plt.ylabel('Preço Ajustado de Fechamento')
plt.legend()
plt.show()

#cálculo de simple moving average de 50 dias
#SMA = simple moving average

#tamanho da janela para a média móvel simples
window_size = 50

plt.figure(figsize=(12, 10))

#loop através de cada empresa para calcular e plotar a média móvel simples
for company in companies:
    company_data = df[df['company'] == company].copy()  #usar .copy() para evitar o SettingWithCopyWarning (permite fazer alterações sem afetar o dataframe original)
    company_data['SMA'] = company_data['adj_close'].rolling(window=window_size).mean() #calcula a média móvel simples para adj_close
    plt.plot(company_data.index, company_data['adj_close'], label=f'{company} - Preço Ajustado')
    plt.plot(company_data.index, company_data['SMA'], label=f'{company} - {window_size}-Day SMA') #plota a média móvel simples calculada

#títulos e rótulos do gráfico
plt.title(f'Médias Móveis Simples de {window_size} Dias ao Longo do Tempo')
plt.xlabel('Data')
plt.ylabel('Preço Ajustado de Fechamento')
plt.legend()
plt.show()

"""## Análise de Correlação"""

#Quais são as correlações entre as ações das diferentes empresas?
#matriz de correlação entre os retornos diários (daily_return) das empresas

#tabela pivô com os retornos diários de cada empresa
pivot_table = df.pivot_table(index='date', columns='company', values='daily_return')
#cada coluna representa uma empresa e cada linha representa uma data, com os valores sendo os retornos diários

#matriz de correlação
correlation_matrix = pivot_table.corr()

print("Matriz de Correlação dos Retornos Diários:")
print(correlation_matrix)

#matriz de correlação usando um heatmap
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)
plt.title('Matriz de Correlação dos Retornos Diários das Ações')
plt.show()

#calcular a correlação de preços ajustados de fechamento (adj_close)

#tabela pivô com os preços ajustados de fechamento de cada empresa
pivot_table_adj_close = df.pivot_table(index='date', columns='company', values='adj_close')

# Calcular a matriz de correlação
correlation_matrix_adj_close = pivot_table_adj_close.corr()

print("Matriz de Correlação dos Preços Ajustados de Fechamento:")
print(correlation_matrix_adj_close)

# Visualizar a matriz de correlação usando um heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix_adj_close, annot=True, cmap='coolwarm', center=0)
plt.title('Matriz de Correlação dos Preços Ajustados de Fechamento das Ações')
plt.show()

#calcular a correlação de preços de abertura (open)

#tabela pivô com os preços de abertura
pivot_table_open = df.pivot_table(index='date', columns='company', values='open')

# Calcular a matriz de correlação
correlation_matrix_open = pivot_table_open.corr()

print("Matriz de Correlação dos Preços de Abertura:")
print(correlation_matrix_open)

# Visualizar a matriz de correlação usando um heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix_open, annot=True, cmap='coolwarm', center=0)
plt.title('Matriz de Correlação dos Preços de Abertura das Ações')
plt.show()

"""##Distribuição de preços das ações

###Histogramas para visualizar a distribuição de preços das ações
"""

#número de bins para o histograma
num_bins = 50

#histograma
plt.figure(figsize=(14, 10))

#loop através de cada empresa para filtrar os dados correspondentes
for i, company in enumerate(companies, 1):
    company_data = df[df['company'] == company]
    plt.subplot(4, 4, i) #subplots para cada empresa (layout de 4x4)
    plt.hist(company_data['adj_close'], bins=num_bins, alpha=0.75, edgecolor='black')
    plt.title(f'{company}')
    plt.xlabel('Preço Ajustado de Fechamento')
    plt.ylabel('Frequência')

#layout
plt.tight_layout()
plt.suptitle('Distribuição dos Preços Ajustados de Fechamento das Ações', y=1.02)
plt.show()

"""###Boxplots para identificar outliers nos dados de preços das ações"""

#boxplots dos Preços Ajustados de Fechamento
plt.figure(figsize=(12, 8))

#loop através de cada empresa, filtrando os dados correspondentes
for i, company in enumerate(companies, 1):
    plt.subplot(4, 4, i)
    company_data = df[df['company'] == company]
    sns.boxplot(y=company_data['adj_close'])
    plt.title(f'{company}')
    plt.xlabel('Preço Ajustado de Fechamento')

#layout e título
plt.tight_layout()
plt.suptitle('Boxplots dos Preços Ajustados de Fechamento das Ações', y=1.02)
plt.show()

"""visualizar a distribuição dos preços ajustados de fechamento entre as 14 empresas"""

#gráfico combinado com todas as empresas
#boxplot individual com Company como eixo X

plt.figure(figsize=(10, 8))
sns.boxplot(data=df, x='company', y='adj_close')
plt.xticks(rotation=90)
plt.title('Boxplots dos Preços Ajustados de Fechamento das Ações')
plt.xlabel('Empresa')
plt.ylabel('Preço Ajustado de Fechamento')
plt.show()

"""#Retorno e Risco

✅Qual empresa oferece os maiores retornos?

*   Netflix possui o maior retorno diário máximo, seguido por Tesla e NVIDIA.

✅Qual empresa apresenta os menores retornos?

*   IBM apresentou o menor retorno mínimo, seguido por Cisco e Intel.

☑ **Média do retorno diário:** 0.0009216606092802763

☑ **Desvio padrão do retorno diário:** 0.02251876598907712

---


✅Quais ações são mais voláteis e, portanto, mais arriscadas?
*   Adobe, Google e Amazon apresentam alta volatilidade e desvio padrão acima de 0.0200.

✅Quais ações são menos voláteis?
*   Apple e IBM são menos voláteis e possuem desvio padrão entre 0.0150 e 0.0200.

☑ **Média da volatilidade:** 0.02168627791077098

☑ **Desvio padrão da volatilidade:** 0.006602319047812381
"""

#medidas de dispersão
#retorno diário, semanal, mensal e anual das ações

df['daily_return'] = df.groupby('company')['adj_close'].pct_change()

print(f'Média do retorno diário: {df["daily_return"].mean()}')
print(f'Desvio padrão do retorno diário: {df["daily_return"].std()}')

#volatilidade intradiária

#volatilidade dos retornos diários
volatility = df.groupby('company')['daily_return'].std()

print(f'Média da volatilidade: {volatility.mean()}')
print(f'Desvio padrão da volatilidade: {volatility.std()}')


#Volatilidade é uma medida de dispersão dos retornos do índice de mercado.
#Quanto mais o preço de uma ação varia em um período curto de tempo, maior o risco de se ganhar ou perder dinheiro negociando esta ação
#a volatilidade é uma medida de risco

"""## Retorno Diário por Empresa"""

#cálculo do retorno diário por empresa
#df['daily_return'] = df.groupby('company')['adj_close'].pct_change()

#visualizar a distribuição do retorno diário para todas as empresas
plt.figure(figsize=(10, 8))
sns.histplot(data=df, x='daily_return', hue='company', bins=50, kde=True, multiple='stack')
plt.title('Distribuição do Retorno Diário por Empresa')
plt.xlabel('Retorno Diário')
plt.ylabel('Frequência')
plt.legend(title='Empresa', labels=df['company'].unique())  #define os rótulos das empresas na legenda
plt.show()

"""**Estatísticas Descritivas do Retorno Diário por Empresa**"""

#import df
df = pd.read_csv('/content/stock_prices_clean.csv')

#'date' no formato datetime
df['date'] = pd.to_datetime(df['date'])

#cálculo do retorno diário por empresa
df['daily_return'] = df.groupby('company')['adj_close'].pct_change()

#calcular estatísticas descritivas do retorno diário por empresa
descriptive_stats = df.groupby('company')['daily_return'].describe()

#exibir estatísticas descritivas
print('Estatísticas Descritivas do Retorno Diário por Empresa:')
print(descriptive_stats)

#visualizar a distribuição do retorno diário para todas as empresas
plt.figure(figsize=(10, 8))
sns.histplot(data=df, x='daily_return', hue='company', bins=50, kde=True, multiple='stack')
plt.title('Distribuição do Retorno Diário por Empresa')
plt.xlabel('Retorno Diário')
plt.ylabel('Frequência')
plt.legend(title='Empresa', labels=df['company'].unique())  #rótulos das empresas na legenda
plt.show()

"""**Standard Deviation => O indicador Desvio Padrão (std)**

### Retorno Diário da Apple
"""

#histograma da distribuição do retorno diário com uma linha de densidade (KDE) para a Apple

#filtra o dataframe original para incluir apenas os dados da Apple
apple_data = df[df['company'] == 'Apple Inc.'].copy()

##calcular o retorno diário, removendo os valores NaN resultantes da mudança percentual
apple_data.loc[:, 'daily_return'] = apple_data['adj_close'].pct_change()
apple_data = apple_data.dropna(subset=['daily_return'])

#calcular a média e o desvio padrão do retorno diário
media_retorno_diario = apple_data['daily_return'].mean()
desvio_padrao_retorno_diario = apple_data['daily_return'].std()

print(f'Média do retorno diário da Apple: {media_retorno_diario}')
print(f'Desvio padrão do retorno diário da Apple: {desvio_padrao_retorno_diario}')

#visualizar a distribuição do retorno diário
plt.figure(figsize=(10, 6))
sns.histplot(apple_data['daily_return'], bins=50, kde=True) #histograma
plt.title('Distribuição do Retorno Diário da Apple')
plt.xlabel('Retorno Diário')
plt.ylabel('Frequência')
plt.show()

"""### Retorno Diário da Adobe"""

#histograma da distribuição do retorno diário com uma linha de densidade (KDE) para a Adobe

#filtra o dataframe original para incluir apenas os dados da Adobe
adobe_data = df[df['company'] == 'Adobe Inc.']

#calcular o retorno diário
adobe_data.loc[:, 'daily_return'] = adobe_data['adj_close'].pct_change()
adobe_data = adobe_data.dropna(subset=['daily_return'])

#calcular a média e o desvio padrão do retorno diário
media_retorno_diario = adobe_data['daily_return'].mean()
desvio_padrao_retorno_diario = adobe_data['daily_return'].std()

print(f'Média do retorno diário da Adobe: {media_retorno_diario}')
print(f'Desvio padrão do retorno diário da Adobe: {desvio_padrao_retorno_diario}')

#visualizar a distribuição do retorno diário
plt.figure(figsize=(10, 6))
sns.histplot(adobe_data['daily_return'], bins=50, kde=True)
plt.title('Distribuição do Retorno Diário da Adobe')
plt.xlabel('Retorno Diário')
plt.ylabel('Frequência')
plt.show()

#print(f"Número de registros para a Adobe: {len(Adobe_data)}")

"""###Retorno Diário da Amazon"""

#histograma da distribuição do retorno diário com uma linha de densidade (KDE) para a Amazon

#filtra o dataframe original para incluir apenas os dados da Amazon
Amazon_data = df[df['company'] == 'Amazon.com, Inc.']

#calcular o retorno diário
Amazon_data.loc[:, 'daily_return'] = Amazon_data['adj_close'].pct_change()
Amazon_data = Amazon_data.dropna(subset=['daily_return'])

#calcular a média e o desvio padrão do retorno diário
media_retorno_diario = Amazon_data['daily_return'].mean()
desvio_padrao_retorno_diario = Amazon_data['daily_return'].std()

print(f'Média do retorno diário da Amazon: {media_retorno_diario}')
print(f'Desvio padrão do retorno diário da Amazon: {desvio_padrao_retorno_diario}')

#visualizar a distribuição do retorno diário
plt.figure(figsize=(10, 6))
sns.histplot(Amazon_data['daily_return'], bins=50, kde=True)
plt.title('Distribuição do Retorno Diário da Amazon')
plt.xlabel('Retorno Diário')
plt.ylabel('Frequência')
plt.show()

"""###Retorno Diário da Salesforce"""

#histograma da distribuição do retorno diário com uma linha de densidade (KDE) para a Salesforce

#filtra o dataframe original para incluir apenas os dados da Salesforce
Salesforce_data = df[df['company'] == 'Salesforce, Inc.'].copy() #garantia que estou lidando com uma cópia independente do dataframe original

#calcular o retorno diário
Salesforce_data.loc[:, 'daily_return'] = Salesforce_data['adj_close'].pct_change()
Salesforce_data = Salesforce_data.dropna(subset=['daily_return'])

#calcular a média e o desvio padrão do retorno diário
media_retorno_diario = Salesforce_data['daily_return'].mean()
desvio_padrao_retorno_diario = Salesforce_data['daily_return'].std()

print(f'Média do retorno diário da Salesforce: {media_retorno_diario}')
print(f'Desvio padrão do retorno diário da Salesforce: {desvio_padrao_retorno_diario}')

#visualizar a distribuição do retorno diário
plt.figure(figsize=(10, 6))
sns.histplot(Salesforce_data['daily_return'], bins=50, kde=True)
plt.title('Distribuição do Retorno Diário da Salesforce')
plt.xlabel('Retorno Diário')
plt.ylabel('Frequência')
plt.show()

"""###Retorno Diário da Cisco Systems"""

#histograma da distribuição do retorno diário com uma linha de densidade (KDE) para a Cisco

#filtra o dataframe original para incluir apenas os dados da Cisco
Cisco_data = df[df['company'] == 'Cisco Systems, Inc.'].copy()

#calcular o retorno diário
Cisco_data.loc[:, 'daily_return'] = Cisco_data['adj_close'].pct_change()
Cisco_data = Cisco_data.dropna(subset=['daily_return'])

#calcular a média e o desvio padrão do retorno diário
media_retorno_diario = Cisco_data['daily_return'].mean()
desvio_padrao_retorno_diario = Cisco_data['daily_return'].std()

print(f'Média do retorno diário da Cisco: {media_retorno_diario}')
print(f'Desvio padrão do retorno diário da Cisco: {desvio_padrao_retorno_diario}')

#visualizar a distribuição do retorno diário
plt.figure(figsize=(10, 6))
sns.histplot(Cisco_data['daily_return'], bins=50, kde=True)
plt.title('Distribuição do Retorno Diário da Cisco')
plt.xlabel('Retorno Diário')
plt.ylabel('Frequência')
plt.show()

"""###Retorno Diário da Alphabet"""

#histograma da distribuição do retorno diário com uma linha de densidade (KDE) para a Alphabet

#filtra o dataframe original para incluir apenas os dados da Alphabet
Alphabet_data = df[df['company'] == 'Alphabet Inc.'].copy()

#calcular o retorno diário
Alphabet_data.loc[:, 'daily_return'] = Alphabet_data['adj_close'].pct_change()
Alphabet_data = Alphabet_data.dropna(subset=['daily_return'])

#calcular a média e o desvio padrão do retorno diário
media_retorno_diario = Alphabet_data['daily_return'].mean()
desvio_padrao_retorno_diario = Alphabet_data['daily_return'].std()

print(f'Média do retorno diário da Alphabet: {media_retorno_diario}')
print(f'Desvio padrão do retorno diário da Alphabet: {desvio_padrao_retorno_diario}')

#visualizar a distribuição do retorno diário
plt.figure(figsize=(10, 6))
sns.histplot(Alphabet_data['daily_return'], bins=50, kde=True)
plt.title('Distribuição do Retorno Diário da Alphabet')
plt.xlabel('Retorno Diário')
plt.ylabel('Frequência')
plt.show()

"""###Retorno Diário da IBM"""

#histograma da distribuição do retorno diário com uma linha de densidade (KDE) para a IBM

#filtra o dataframe original para incluir apenas os dados da IBM
IBM_data = df[df['company'] == 'International Business Machines Corporation'].copy()

#calcular o retorno diário
IBM_data.loc[:, 'daily_return'] = IBM_data['adj_close'].pct_change()
IBM_data = IBM_data.dropna(subset=['daily_return'])

#calcular a média e o desvio padrão do retorno diário
media_retorno_diario = IBM_data['daily_return'].mean()
desvio_padrao_retorno_diario = IBM_data['daily_return'].std()

print(f'Média do retorno diário da IBM: {media_retorno_diario}')
print(f'Desvio padrão do retorno diário da IBM: {desvio_padrao_retorno_diario}')

#visualizar a distribuição do retorno diário
plt.figure(figsize=(10, 6))
sns.histplot(IBM_data['daily_return'], bins=50, kde=True)
plt.title('Distribuição do Retorno Diário da IBM')
plt.xlabel('Retorno Diário')
plt.ylabel('Frequência')
plt.show()

"""###Retorno Diário da Intel"""

#histograma da distribuição do retorno diário com uma linha de densidade (KDE) para a Intel

#filtra o dataframe original para incluir apenas os dados da Intel
Intel_data = df[df['company'] == 'Intel Corporation'].copy()

#calcular o retorno diário
Intel_data.loc[:, 'daily_return'] = Intel_data['adj_close'].pct_change()
Intel_data = Intel_data.dropna(subset=['daily_return'])

#calcular a média e o desvio padrão do retorno diário
media_retorno_diario = Intel_data['daily_return'].mean()
desvio_padrao_retorno_diario = Intel_data['daily_return'].std()

print(f'Média do retorno diário da Intel: {media_retorno_diario}')
print(f'Desvio padrão do retorno diário da Intel: {desvio_padrao_retorno_diario}')

#visualizar a distribuição do retorno diário
plt.figure(figsize=(10, 6))
sns.histplot(Intel_data['daily_return'], bins=50, kde=True)
plt.title('Distribuição do Retorno Diário da Intel')
plt.xlabel('Retorno Diário')
plt.ylabel('Frequência')
plt.show()

"""###Retorno Diário da Meta"""

#histograma da distribuição do retorno diário com uma linha de densidade (KDE) para a Meta

#filtra o dataframe original para incluir apenas os dados da Meta
Meta_data = df[df['company'] == 'Meta Platforms, Inc.'].copy()

#calcular o retorno diário
Meta_data.loc[:, 'daily_return'] = Meta_data['adj_close'].pct_change()
Meta_data = Meta_data.dropna(subset=['daily_return'])

#calcular a média e o desvio padrão do retorno diário
media_retorno_diario = Meta_data['daily_return'].mean()
desvio_padrao_retorno_diario = Meta_data['daily_return'].std()

print(f'Média do retorno diário da Meta: {media_retorno_diario}')
print(f'Desvio padrão do retorno diário da Meta: {desvio_padrao_retorno_diario}')

#visualizar a distribuição do retorno diário
plt.figure(figsize=(10, 6))
sns.histplot(Meta_data['daily_return'], bins=50, kde=True)
plt.title('Distribuição do Retorno Diário da Meta')
plt.xlabel('Retorno Diário')
plt.ylabel('Frequência')
plt.show()

"""###Retorno Diário da Microsoft"""

#histograma da distribuição do retorno diário com uma linha de densidade (KDE) para a Microsoft

#filtra o dataframe original para incluir apenas os dados da Microsoft
Microsoft_data = df[df['company'] == 'Microsoft Corporation'].copy()

#calcular o retorno diário
Microsoft_data.loc[:, 'daily_return'] = Microsoft_data['adj_close'].pct_change()
Microsoft_data = Microsoft_data.dropna(subset=['daily_return'])

#calcular a média e o desvio padrão do retorno diário
media_retorno_diario = Microsoft_data['daily_return'].mean()
desvio_padrao_retorno_diario = Microsoft_data['daily_return'].std()

print(f'Média do retorno diário da Microsoft: {media_retorno_diario}')
print(f'Desvio padrão do retorno diário da Microsoft: {desvio_padrao_retorno_diario}')

#visualizar a distribuição do retorno diário
plt.figure(figsize=(10, 6))
sns.histplot(Microsoft_data['daily_return'], bins=50, kde=True)
plt.title('Distribuição do Retorno Diário da Microsoft')
plt.xlabel('Retorno Diário')
plt.ylabel('Frequência')
plt.show()

"""###Retorno Diário da Netflix"""

#histograma da distribuição do retorno diário com uma linha de densidade (KDE) para a Netflix

#filtra o dataframe original para incluir apenas os dados da Netflix
Netflix_data = df[df['company'] == 'Netflix, Inc.'].copy()

#calcular o retorno diário
Netflix_data.loc[:, 'daily_return'] = Netflix_data['adj_close'].pct_change()
Netflix_data = Netflix_data.dropna(subset=['daily_return'])

#calcular a média e o desvio padrão do retorno diário
media_retorno_diario = Netflix_data['daily_return'].mean()
desvio_padrao_retorno_diario = Netflix_data['daily_return'].std()

print(f'Média do retorno diário da Netflix: {media_retorno_diario}')
print(f'Desvio padrão do retorno diário da Netflix: {desvio_padrao_retorno_diario}')

#visualizar a distribuição do retorno diário
plt.figure(figsize=(10, 6))
sns.histplot(Netflix_data['daily_return'], bins=50, kde=True)
plt.title('Distribuição do Retorno Diário da Netflix')
plt.xlabel('Retorno Diário')
plt.ylabel('Frequência')
plt.show()

"""###Retorno Diário da NVIDIA"""

#histograma da distribuição do retorno diário com uma linha de densidade (KDE) para a NVIDIA

#filtra o dataframe original para incluir apenas os dados da NVIDIA
NVIDIA_data = df[df['company'] == 'NVIDIA Corporation'].copy()

#calcular o retorno diário
NVIDIA_data.loc[:, 'daily_return'] = NVIDIA_data['adj_close'].pct_change()
NVIDIA_data = NVIDIA_data.dropna(subset=['daily_return'])

#calcular a média e o desvio padrão do retorno diário
media_retorno_diario = NVIDIA_data['daily_return'].mean()
desvio_padrao_retorno_diario = NVIDIA_data['daily_return'].std()

print(f'Média do retorno diário da NVIDIA: {media_retorno_diario}')
print(f'Desvio padrão do retorno diário da NVIDIA: {desvio_padrao_retorno_diario}')

#visualizar a distribuição do retorno diário
plt.figure(figsize=(10, 6))
sns.histplot(NVIDIA_data['daily_return'], bins=50, kde=True)
plt.title('Distribuição do Retorno Diário da NVIDIA')
plt.xlabel('Retorno Diário')
plt.ylabel('Frequência')
plt.show()

"""###Retorno Diário da Oracle"""

#histograma da distribuição do retorno diário com uma linha de densidade (KDE) para a Oracle

#filtra o dataframe original para incluir apenas os dados da Oracle
Oracle_data = df[df['company'] == 'Oracle Corporation'].copy()

#calcular o retorno diário
Oracle_data.loc[:, 'daily_return'] = Oracle_data['adj_close'].pct_change()
Oracle_data = Oracle_data.dropna(subset=['daily_return'])

#calcular a média e o desvio padrão do retorno diário
media_retorno_diario = Oracle_data['daily_return'].mean()
desvio_padrao_retorno_diario = Oracle_data['daily_return'].std()

print(f'Média do retorno diário da Oracle: {media_retorno_diario}')
print(f'Desvio padrão do retorno diário da Oracle: {desvio_padrao_retorno_diario}')

#visualizar a distribuição do retorno diário
plt.figure(figsize=(10, 6))
sns.histplot(Oracle_data['daily_return'], bins=50, kde=True)
plt.title('Distribuição do Retorno Diário da Oracle')
plt.xlabel('Retorno Diário')
plt.ylabel('Frequência')
plt.show()

"""### Retorno Diário da Tesla"""

#histograma da distribuição do retorno diário com uma linha de densidade (KDE) para a Tesla

#filtra o dataframe original para incluir apenas os dados da Tesla
tesla_data = df[df['company'] == 'Tesla, Inc.']

#calcular o retorno diário
tesla_data.loc[:, 'daily_return'] = tesla_data['adj_close'].pct_change()
#.pct_change() calcula o retorno percentual de mudança para a coluna 'adj_close' dentro de cada grupo (para cada empresa)

#calcular a média e o desvio padrão do retorno diário
media_retorno_diario = tesla_data['daily_return'].mean()
desvio_padrao_retorno_diario = tesla_data['daily_return'].std()

print(f'Média do retorno diário da Tesla: {media_retorno_diario}')
print(f'Desvio padrão do retorno diário da Tesla: {desvio_padrao_retorno_diario}')

#visualizar a distribuição do retorno diário
plt.figure(figsize=(8, 4))
sns.histplot(tesla_data['daily_return'].dropna(), bins=50, kde=True) #histograma
plt.title('Distribuição do Retorno Diário da Tesla')
plt.xlabel('Retorno Diário')
plt.ylabel('Frequência')
plt.show()

"""## Retorno Mensal por Empresa"""

#calcular os retornos mensais para cada empresa
df['monthly_return'] = df.groupby(['company', df['date'].dt.to_period('M')])['adj_close'].pct_change()

#gráfico dos retornos mensais
plt.figure(figsize=(12, 8))
sns.lineplot(data=df, x='date', y='monthly_return', hue='company', estimator=None)
plt.title('Retornos Mensais das 14 Empresas')
plt.xlabel('Data')
plt.ylabel('Retorno Mensal')
plt.legend(title='Empresa', labels=df['company'].unique(), loc='upper left', bbox_to_anchor=(1, 1))
plt.show()

"""## Retorno Anual por Empresa"""

#calcular os retornos anuais para cada empresa
df['year'] = df['date'].dt.year #extrai o ano da coluna de data para agrupamento por ano
df['annual_return'] = df.groupby(['company', 'year'])['adj_close'].pct_change() #agrupa os dados pelo nome da empresa e pelo ano da data
#.pct_change() calcula o retorno percentual de mudança para os preços ajustados de fechamento (adj_close) dentro de cada grupo (empresa e ano)


#gráfico dos retornos anuais
plt.figure(figsize=(12, 8))
sns.lineplot(data=df, x='year', y='annual_return', hue='company', estimator=None) #função do Seaborn para criar gráfico de linha
plt.title('Retornos Anuais das 14 Empresas')
plt.xlabel('Ano')
plt.ylabel('Retorno Anual')
plt.legend(title='Empresa', labels=df['company'].unique(), loc='upper left', bbox_to_anchor=(1, 1))
plt.show()

"""# Análise de Risco

O cálculo do Valor em Risco (VaR) é uma importante medida de risco financeiro para avaliar a probabilidade de perdas significativas. Nessa análise de risco foi usada uma abordagem histórica para calcular o VaR, que considera os retornos históricos para estimar o potencial de perda em um determinado horizonte de tempo, dado um nível de confiança específico.

Foi considerado um nível de confiança de 95% e um horizonte de tempo de 1 dia.

*   O VaR a 95% de confiança para os últimos 5 anos fornece uma visão do risco potencial de perda anual para cada empresa.

*   Empresas com VaR mais alto apresentam maior risco de perdas anuais inesperadas.
"""

#import df
df_original = pd.read_csv('/content/stock_prices_clean.csv')

#'date' no formato datetime
df_original['date'] = pd.to_datetime(df_original['date'])

#criar df_copy do df_original
df_copy = df_original.copy()

#filtrar os últimos 5 anos (2019-2023)
end_date = df_copy['date'].max()
start_date = end_date - pd.DateOffset(years=5)
df_copy = df_copy[(df_copy['date'] >= start_date) & (df_copy['date'] <= end_date)]

#retornos diários calculados para cada empresa
#cálculo do retorno diário por empresa
df_copy['daily_return'] = df_copy.groupby('company')['adj_close'].pct_change()

#O VaR é calculado para cada empresa usando a função calcular_var_historico
#função para calcular o VaR histórico
def calcular_var_historico(retornos, confianca=0.95):
    #ordenar os retornos
    retornos_ordenados = np.sort(retornos.dropna())

    #calcular o percentil do VaR
    indice_var = int((1 - confianca) * len(retornos_ordenados))
    var = retornos_ordenados[indice_var]

    return var

#calcular o VaR para cada empresa
var_por_empresa = df_copy.groupby('company')['daily_return'].apply(lambda x: calcular_var_historico(x, 0.95))

#criar novo dfe para armazenar o resultado
valor_em_risco = var_por_empresa.reset_index()
valor_em_risco.columns = ['company', 'VaR_95']

#salvar os resultados em uma nova tabela chamada valor_em_risco
valor_em_risco.to_csv('/content/valor_em_risco.csv', index=False)

#exibir o VaR calculado para cada empresa
print('Valor em Risco (VaR) a 95% de confiança para cada empresa:')
print(valor_em_risco)

#visualizar o VaR por empresa em um gráfico de barras
plt.figure(figsize=(14, 6))
plt.bar(valor_em_risco['company'], valor_em_risco['VaR_95'], color='skyblue')
plt.title('Valor em Risco (VaR) a 95% de Confiança por Empresa nos Últimos 5 Anos')
plt.xlabel('Empresa')
plt.ylabel('VaR')
plt.xticks(rotation=45)
plt.show()

"""*    **NVIDIA, Netflix e Tesla:** Apresentam VaR relativamente alto, sugerindo maior risco potencial de perdas anuais.
*    **IBM e Oracle:** Possuem VaR mais baixo, indicando menor risco potencial de perdas anuais.

Todos os valores de VaR são negativos, indicando perdas potenciais.

O VaR varia entre -0.0258 (IBM) e -0.0642 (Tesla), o que significa que a perda máxima anual esperada varia entre 2,58% e 6,42%.

Empresas com maior volatilidade diária tendem a ter uma diferença maior entre os VaRs.

---

**Observações sobre a tendências ao longo dos anos:**
*    **Adobe, Amazon, Apple, Microsoft e Salesforce:** Apresentaram uma tendência de aumento nos últimos 5 anos, indicando um aumento no risco das empresas.

*    **Cisco, Intel, IBM, Oracle e Tesla:** Apresentaram uma tendência de diminuição.


O VaR anual apresentado é maior que o VaR diário (anteriormente analisado).

# Análise Técnica

O objetivo da aplicação das técnicas de análise é identificar as ações das 14 empresas com o maior crescimento no preço ajustado de fechamento ao longo dos anos, utilizando a taxa de crescimento anual composta (CAGR) como métrica de avaliação, e avaliar a volatilidade e o risco dessas ações, para que um novo investidor possa tomar decisões informadas sobre investimentos com bom histórico de crescimento e menor variação.

Métricas para crescimento:
*   Taxa de crescimento anual composta (CAGR) do preço ajustado de fechamento
*   Desempenho anualizado

Métricas para risco:
*   Desvio padrão dos retornos diários (volatilidade)
*   Valor em risco (VaR)

A CAGR (do inglês Compound Annual Growth Rate) é uma métrica utilizada para calcular a taxa média de crescimento de um investimento, levando em consideração a volatilidade e as flutuações ao longo do tempo.

## CAGR para cada empresa
"""

#cálculo da CAGR
def calculate_cagr(data, start_date, end_date):
    data_period = data[(data['date'] >= start_date) & (data['date'] <= end_date)]
    start_value = data_period.iloc[0]['adj_close']
    end_value = data_period.iloc[-1]['adj_close']
    periods = (data_period['date'].iloc[-1] - data_period['date'].iloc[0]).days / 365.25
    cagr = (end_value / start_value) ** (1 / periods) - 1
    return cagr

#aplicar o cálculo da CAGR para cada empresa
for company in df['company'].unique():
    company_data = df[df['company'] == company]
    cagr = calculate_cagr(company_data, '2019-01-01', '2023-12-31')
    print(f'CAGR da {company}: {cagr:.2%}')

"""**Variações significativas no CAGR (Taxa de Crescimento Anual Composta) de 5 anos para estas empresas:**

*   **Alto Crescimento:** Tesla (55.99%), NVIDIA  (44.31%), Apple (35.92%) e Microsoft (25.67%) lideram o grupo com CAGR acima de 25%.

*   **Crescimento Moderado:** Alphabet (13.84%), Oracle (17.84%), Cisco (5.73%), Adobe (10.76%) e IBM (11.74%) apresentam crescimento moderado entre 5% e 18%.

*   **Estagnação/Declínio:** Meta (1.33%), Netflix (2.13%), Amazon (2.27%), Salesforce (-0.56%) e Intel (-11.16%) exibem crescimento estagnado ou negativo no período.

✅**Considerando apenas o CAGR, Tesla, Nvidia e Apple se destacam.**
"""

#Calcular CAGR para cada empresa por ano
#função para calcular CAGR

def calculate_annual_cagr(df, start_year, end_year):
    cagr_dict = {}
    for company in df['company'].unique():
        company_data = df[df['company'] == company]
        cagr_values = []
        for year in range(start_year, end_year):
            start_date = f'{year}-01-01'
            end_date = f'{year}-12-31'
            data_period = company_data[(company_data['date'] >= start_date) & (company_data['date'] <= end_date)]
            if not data_period.empty:
                start_value = data_period.iloc[0]['adj_close']
                end_value = data_period.iloc[-1]['adj_close']
                if start_value != 0:
                    cagr = (end_value / start_value) ** (1 / 1) - 1
                    cagr_values.append(cagr)
        cagr_dict[company] = cagr_values
    return cagr_dict

#calcular CAGR de 2019 a 2023 (úlitmos 5 anos)
cagr_data = calculate_annual_cagr(df, 2019, 2024)

#Preparar os dados para ANOVA convertendo os dados de CAGR calculados em formato que possa ser usado para o teste ANOVA

# Converter o dicionário de CAGR em um dataframe
cagr_list = []
for company, cagr_values in cagr_data.items():
    for cagr in cagr_values:
        cagr_list.append({'company': company, 'cagr': cagr})

cagr_df = pd.DataFrame(cagr_list)

#Realizar o teste ANOVA com os dados prontos
from scipy.stats import f_oneway

#criar lista de CAGRs por empresa
groups = [cagr_df[cagr_df['company'] == company]['cagr'] for company in cagr_df['company'].unique()]

#ANOVA
anova_result = f_oneway(*groups)

print(f"Estatística do Teste F: {anova_result.statistic}")
print(f"P-valor: {anova_result.pvalue}")

#resultado
alpha = 0.05  #nível de significância
if anova_result.pvalue < alpha:
  #se o p-valor do teste for menor que o nível de significância (0.05), há uma diferença significativa entre as CAGRs das empresas
    print("Resultado: Rejeitamos a hipótese nula. Há diferença significativa nas CAGRs entre as empresas.")
else:
  #caso contrário, não há evidências suficientes para afirmar que as CAGRs das empresas são diferentes
    print("Resultado: Não há evidências suficientes para rejeitar a hipótese nula.")

import matplotlib.pyplot as plt

#dados de CAGR
companies = [
    'Apple Inc.', 'Adobe Inc.', 'Amazon.com, Inc.', 'Salesforce, Inc.', 'Cisco Systems, Inc.',
    'Alphabet Inc.', 'IBM Corporation', 'Intel Corporation',
    'Meta Platforms, Inc.', 'Microsoft Corporation', 'Netflix, Inc.', 'NVIDIA Corporation',
    'Oracle Corporation', 'Tesla, Inc.'
]

cagrs = [
    35.92, 10.76, 2.27, -0.56, 5.73, 13.84, 11.74, -11.16, 1.33, 25.67, 2.13, 44.31, 17.84, 55.99
]

#criar dataframe para os CAGRs
cagr_df = pd.DataFrame({
    'company': companies,
    'cagr': cagrs
})

#gráfico de barras
plt.figure(figsize=(10, 8))
plt.barh(cagr_df['company'], cagr_df['cagr'], color='skyblue')
plt.xlabel('CAGR (%)')
plt.title('CAGR das Empresas')
plt.grid(axis='x')
plt.show()

# Estatísticas descritivas
cagr_descriptive = cagr_df['cagr'].describe()
print(cagr_descriptive)

print(f'Média CAGR: {cagr_descriptive["mean"]:.2%}')
print(f'Desvio Padrão CAGR: {cagr_descriptive["std"]:.2%}')

"""A análise do CAGR pode complementar outras técnicas de análise fundamental e técnica para fornecer uma compreensão mais completa das perspectivas e valor potencial de uma empresa.

Segmentar empresas com base no CAGR permite a comparação direta de suas trajetórias de crescimento. Os investidores podem identificar empresas que superaram ou ficaram abaixo da média do mercado, o que pode informar decisões de investimento e estratégias de alocação de setores.

### Hipóteses

Análise dos Retornos Diários com  Teste ANOVA (Análise de Variância):

A análise de daily_return verifica se há diferenças significativas entre as empresas em termos de seus retornos diários.
"""

#teste de significância para determinar se os resultados do teste ANOVA são significativos para os retornos diários

from scipy.stats import f_oneway

#realizar teste ANOVA para comparar retornos médios das 14 empresas
anova_result = f_oneway(*groups)

print(f"Estatística do Teste F: {anova_result.statistic}")
print(f"P-valor: {anova_result.pvalue}")

#resultado
alpha = 0.05
if anova_result.pvalue < alpha:
    print("Resultado: Rejeitamos a hipótese H0. Há diferença significativa nos retornos diários médios entre as empresas.")
else:
    print("Resultado: Não há evidências suficientes para rejeitar a hipótese H0.")
    #Não há evidências suficientes para rejeitar a hipótese de que as médias dos CAGRs são iguais entre todas as empresas.

"""**1.   Hipótese (H0): As médias dos retornos diários médios são iguais entre todas as empresas.**

► Não foi rejeitada.

A hipótese de que as médias dos CAGRs são iguais entre todas as empresas não foi rejeitada com base nos dados e no teste ANOVA realizado, pois não há evidências estatísticas suficientes para concluir que há uma diferença significativa nas médias dos CAGRs entre as empresas.

---

**2.   Hipótese (H1): Pelo menos uma das médias dos CAGRs é diferente entre as empresas.**

►Não foi validada.

A hipótese de que pelo menos uma das médias dos CAGRs é diferente entre as empresas não foi validada, pois não foi encontrada uma diferença significativa entre os CAGRs das empresas.

---
**Resultados**

*   Estatística do Teste F: 0.8141268350375086
*   P-valor: 0.6427312816103227


**Conclusão**

Com base nos resultados do teste ANOVA (estatística F e p-valor), não há evidências suficientes para rejeitar a hipótese H0 de que as médias dos CAGRs das 14 empresas são iguais. Portanto, a hipótese foi mantida, indicando que as diferenças observadas nos CAGRs podem ser explicadas pela variabilidade dentro das empresas, e não por diferenças reais entre as empresas.
Como o nível de significância comum utilizado em testes estatísticos é 0.05 e o p-valor é maior (0.64), não há evidências suficientes para rejeitar a hipótese H0.

# Segmentação

Os dados foram segmentados para identificar padrões em diferentes grupos.

A segmentação dos preços de ações de 14 empresas entre 2019 e 2023 oferece diversos benefícios no contexto de compreender tendências de mercado, identificar oportunidades de investimento e avaliar o desempenho das empresas.

A seguir vamos explorar algumas segmentações:

### Segmentação por Volume de Negociação
"""

#carregar o df original do arquivo CSV
df_original = pd.read_csv('/content/stock_prices_clean.csv')

#criar cópia do DF original
df_copy = df_original.copy()

#função para segmentar o volume de negociação
def segmentar_volume(volume):
    if volume > 1000000:
        return 'Alto Volume'
    elif 500000 < volume <= 1000000:
        return 'Volume Moderado'
    else:
        return 'Baixo Volume'

#calcular o volume médio por empresa
volume_medio = df_copy.groupby('company')['volume'].mean().reset_index()
volume_medio.columns = ['company', 'avg_volume']

#unir o volume médio ao df_copy
df_copy = df_copy.merge(volume_medio, on='company', how='left')

#aplicar a segmentação
df_copy['segmento_volume'] = df_copy['avg_volume'].apply(segmentar_volume)

#salvar os resultados em uma nova tabela chamada segmentacao_volume
segmentacao_volume = df_copy[['company', 'avg_volume', 'segmento_volume']].drop_duplicates().reset_index(drop=True)
segmentacao_volume.to_csv('/content/segmentacao_volume.csv', index=False)

#exibir os resultados descritos
for index, row in segmentacao_volume.iterrows():
    print(f"Empresa: {row['company']}, Volume Médio: {row['avg_volume']:.2f}, Segmento: {row['segmento_volume']}")

#visualização com gráfico de barras
plt.figure(figsize=(10, 8))
sns.barplot(x='company', y='avg_volume', hue='segmento_volume', data=segmentacao_volume, palette='viridis')
plt.title('Volume Médio de Negociação por Empresa e Segmento')
plt.xlabel('Empresa')
plt.ylabel('Volume Médio')
plt.xticks(rotation=45)
plt.legend(title='Segmento de Volume')
plt.show()

"""
*   **Liquidez Geral do Mercado:**

O fato de todas as 14 empresas estarem na categoria "Alto Volume" indica que o mercado geral para essas ações de tecnologia é líquido. Isso significa que os novos investidores devem poder comprar e vender ações dessas empresas com relativa facilidade, sem impactar significativamente o preço.

*   **Liquidez Relativa de Cada Empresa:**

Dentro do segmento "Alto Volume", ainda existe uma ampla faixa de volumes médios de transação entre as empresas. A Tesla tem um volume médio superior a 935 milhões, enquanto a Adobe tem um volume médio inferior a 4 milhões. Isso indica que as ações da Tesla são mais líquidas do que as da Adobe."""

df_seg_volume = pd.read_csv('/content/segmentacao_volume.csv')
print(df_seg_volume.info())
print(df_seg_volume.dtypes)
print(df_seg_volume.head())

"""### Segmentação por Crescimento do CAGR

A segmentação por CAGR permite identificar empresas com alto, moderado ou baixo potencial de crescimento.
"""

#carregar o DataFrame original do arquivo CSV
#df_original = pd.read_csv('/content/stock_prices_clean.csv')

#criar cópia do df original
df_copy = df_original.copy()

#dados do CAGR para cada empresa
cagr_data = {
    'company': ['Apple Inc.', 'Adobe Inc.', 'Amazon.com, Inc.', 'Salesforce, Inc.', 'Cisco Systems, Inc.',
                'Alphabet Inc.', 'International Business Machines Corporation', 'Intel Corporation',
                'Meta Platforms, Inc.', 'Microsoft Corporation', 'Netflix, Inc.', 'NVIDIA Corporation',
                'Oracle Corporation', 'Tesla, Inc.'],
    'cagr': [35.92, 10.76, 2.27, -0.56, 5.73, 13.84, 11.74, -11.16, 1.33, 25.67, 2.13, 44.31, 17.84, 55.99]
}

#criação do DataFrame de CAGR
df_cagr = pd.DataFrame(cagr_data)

#função para segmentar o crescimento do CAGR
def segmentar_cagr(cagr):
    if cagr > 20:
        return 'Alto Crescimento'
    elif 10 < cagr <= 20:
        return 'Crescimento Moderado'
    else:
        return 'Baixo Crescimento'

#segmentação ao DataFrame de CAGR
df_cagr['segmento_cagr'] = df_cagr['cagr'].apply(segmentar_cagr)

#merge de df_copy com df_cagr
df_copy = df_copy.merge(df_cagr[['company', 'cagr', 'segmento_cagr']], on='company', how='left')

#salvar os resultados em uma nova tabela chamada segmentacao_cagr
df_copy.to_csv('/content/segmentacao_cagr.csv', index=False)

#resultados descritos
for index, row in df_cagr.iterrows():
    print(f"Empresa: {row['company']}, CAGR: {row['cagr']:.2f}%, Segmento: {row['segmento_cagr']}")

#visualização com gráfico de barras
plt.figure(figsize=(14, 6))
sns.barplot(x='company', y='cagr', hue='segmento_cagr', data=df_cagr, palette='rainbow')
plt.title('CAGR por Empresa e Segmento')
plt.xlabel('Empresa')
plt.ylabel('CAGR (%)')
plt.xticks(rotation=45)
plt.legend(title='Segmento de Crescimento')
plt.show()

"""*     **Alto Crescimento:**
As empresas Apple, Tesla, NVIDIA, Microsoft apresentaram CAGR significativamente acima da média do mercado, demonstrando forte desempenho nos últimos 5 anos.
*    **Crescimento Moderado:**
As empresas Alphabet, IBM, Oracle, Adobe apresentaram CAGR consistente, mas abaixo do ritmo das empresas de alto crescimento.
*    **Baixo Crescimento:**
As empresas Amazon, Salesforce, Cisco, Meta, Netflix, Intel apresentaram CAGR estagnado ou negativo, indicando potencial necessidade de reestruturação ou mudança de estratégia.

###Segmentação por Volatilidade

Para essa segmentação é utlizado o cálculo do desvio padrão que mede a variabilidade dos retornos das ações de uma empresa em torno de sua média.

A segmentação por desvio padrão permite identificar empresas com alta, moderada ou baixa volatilidade.
"""

#carregar o df original do arquivo CSV
df_original = pd.read_csv('/content/stock_prices_clean.csv')

#criar cópia do DF original
df_copy = df_original.copy()

#função para segmentar a volatilidade
def segmentar_volatilidade(std):
    if std > 0.02:
        return 'Alta Volatilidade'
    elif 0.01 < std <= 0.02:
        return 'Volatilidade Moderada'
    else:
        return 'Baixa Volatilidade'

#calcular o desvio padrão dos retornos diários por empresa
volatilidade = df_copy.groupby('company')['daily_return'].std().reset_index()
volatilidade.columns = ['company', 'std']

#unir a volatilidade ao df_copy
df_copy = pd.merge(df_copy, volatilidade, on='company')

#aplicar a segmentação
df_copy['segmento_volatilidade'] = df_copy['std'].apply(segmentar_volatilidade)

#salvar os resultados em uma nova tabela chamada segmentacao_volatilidade
df_copy.to_csv('/content/segmentacao_volatilidade.csv', index=False)

#resultados descritos
for index, row in volatilidade.iterrows():
    segmento = segmentar_volatilidade(row['std'])
    print(f"Empresa: {row['company']}, Desvio Padrão: {row['std']:.4f}, Segmento: {segmento}")

#visualização com gráfico de barras
plt.figure(figsize=(14, 6))
sns.barplot(x='company', y='std', hue='segmento_volatilidade', data=df_copy, palette='coolwarm')
plt.title('Volatilidade dos Retornos Diários por Empresa e Segmento')
plt.xlabel('Empresa')
plt.ylabel('Desvio Padrão dos Retornos Diários')
plt.xticks(rotation=45)
plt.legend(title='Segmento de Volatilidade')
plt.show()

"""**Alta Volatilidade:** Desvio padrão acima de 0.0200

*   Um desvio padrão alto indica que o preço da ação pode oscilar significativamente em um curto período de tempo.

**Volatilidade Moderada:** Desvio padrão entre 0.0150 e 0.0200

*   Um desvio padrão baixo significa que o preço da ação é mais estável.

---
A volatilidade representa a dispersão dos retornos das ações, não o risco em si. Ações voláteis podem ter retornos altos ou baixos, enquanto ações menos voláteis podem ter retornos consistentes, mas potencialmente mais baixos.

A análise da volatilidade complementa as outras métricas financeiras e análises técnicas para fornecer uma compreensão mais completa do perfil de risco e retorno de uma empresa.

A tolerância ao risco é um fator crucial para o sucesso do investimento, pois ao compreender a volatilidade de diferentes empresas, os novos investidores podem escolher ativos que se alinham com seu perfil de risco individual.
"""

df_seg_volatilidade = pd.read_csv('/content/segmentacao_volatilidade.csv')
print(df_seg_volatilidade.info())
print(df_seg_volatilidade.dtypes)
print(df_seg_volatilidade.head())

"""###Segmentação por Setor

A segmentação por setor, com a adição de mais contexto sobre as empresas, fornece uma visão geral do CAGR e da volatilidade anualizada para cada empresa e permite identificar empresas com diferentes características de crescimento e risco.
"""

#análise do CAGR (Taxa de Crescimento Anual Composta) e a volatilidade das ações de 14 empresas durante os últimos 5 anos e segmentação das empresas por setor

#import df
df = pd.read_csv('/content/stock_prices_clean.csv')

#convertendo colunas para tipos corretos
df['date'] = pd.to_datetime(df['date']) #converte date para datetime do Pandas

#Dicionário contendo o setor de cada símbolo de ação
#adc informações de setor
setor_info = {
    'AAPL': 'Tecnologia',
    'ADBE': 'Tecnologia',
    'AMZN': 'Consumo e Varejo',
    'CRM': 'Tecnologia',
    'CSCO': 'Tecnologia',
    'GOOGL': 'Serviços de comunicação',
    'IBM': 'Tecnologia',
    'INTC': 'Tecnologia',
    'META': 'Serviços de comunicação',
    'MSFT': 'Tecnologia',
    'NFLX': 'Serviços de comunicação',
    'NVDA': 'Tecnologia',
    'ORCL': 'Tecnologia',
    'TSLA': 'Automotivo'
}

#Dicionário contendo o nome completo da empresa para cada símbolo de ação
#símbolos de ação para os nomes das empresas
company_info = {
    'AAPL': 'Apple Inc.',
    'ADBE': 'Adobe Inc.',
    'AMZN': 'Amazon.com, Inc.',
    'CRM': 'Salesforce, Inc.',
    'CSCO': 'Cisco Systems, Inc.',
    'GOOGL': 'Alphabet Inc.',
    'IBM': 'International Business Machines Corporation',
    'INTC': 'Intel Corporation',
    'META': 'Meta Platforms, Inc.',
    'MSFT': 'Microsoft Corporation',
    'NFLX': 'Netflix, Inc.',
    'NVDA': 'NVIDIA Corporation',
    'ORCL': 'Oracle Corporation',
    'TSLA': 'Tesla, Inc.'
}

#adc informações de empresa e setor
df['company'] = df['stock_symbol'].map(company_info) #nova coluna company mapeando cada símbolo de ação para o setor da empresa do dicionário company_info
df['setor'] = df['stock_symbol'].map(setor_info) #nova coluna setor mapeando cada símbolo de ação para o setor da empresa do dicionário setor_info

#cópia do dataframe original para a análise para evitar modificar o dataframe original durante a análise
df_copy = df.copy()

#filtrar os últimos 5 anos de dados
end_date = df_copy['date'].max() #obtém a data mais recente no dataframe df_copy
start_date = end_date - pd.DateOffset(years=5) #calcula a data 5 anos antes da data mais recente
df_5_years = df_copy[df_copy['date'] >= start_date] #filtra o df_copy para incluir apenas dados a partir da data de 5 anos atrás

#Funções para cálculo de CAGR e Volatilidade:
    #função para calcular CAGR
    def calculate_cagr(df, symbol):
        df_symbol = df[df['stock_symbol'] == symbol]
        if len(df_symbol) == 0:
            return np.nan
        start_price = df_symbol['adj_close'].iloc[0]
        end_price = df_symbol['adj_close'].iloc[-1]
        n = len(df_symbol) / 252  #aprox 252 dias úteis por ano
        cagr = (end_price / start_price) ** (1 / n) - 1
        return cagr #retorna o CAGR do símbolo de ação, se houver dados disponíveis, ou NaN caso contrário

    #função para calcular a volatilidade anualizada
    def calculate_volatility(df, symbol):
        df_symbol = df[df['stock_symbol'] == symbol]
        if len(df_symbol) == 0:
            return np.nan
        daily_returns = df_symbol['daily_return'] #calcula a volatilidade diária dos retornos
        volatility = np.std(daily_returns) * np.sqrt(252)  # Annualizando a volatilidade
        return volatility #retorna a volatilidade anualizada do símbolo de ação, se houver dados disponíveis, ou NaN caso contrário

#cálculo de CAGR e Volatilidade para todas as empresas
cagr_list = [] #cria uma lista vazia para armazenar os valores de CAGR
volatility_list = [] #cria uma lista vazia para armazenar os valores de volatilidade
symbols = df_5_years['stock_symbol'].unique() #obtém a lista de símbolos de ações únicos presentes no dataframe filtrado

for symbol in symbols:
    cagr = calculate_cagr(df_5_years, symbol) #função para calcular o CAGR
    volatility = calculate_volatility(df_5_years, symbol) #função para calcular a volatilidade anualizada
    cagr_list.append(cagr) #adc os valores calculados de CAGR a lista cagr_list
    volatility_list.append(volatility) #adc os valores calculados de volatilidade a lista volatility_list

#criação de novo dataframe para armazenar as métricas com 3 colunas
metrics_df = pd.DataFrame({'stock_symbol': symbols, 'cagr': cagr_list, 'volatility': volatility_list})

#adc informações de empresa e setor ao novo dataframe
metrics_df['company'] = metrics_df['stock_symbol'].map(company_info)
metrics_df['setor'] = metrics_df['stock_symbol'].map(setor_info)

#verificar se há valores NaN no dataframe de métricas
print(metrics_df.isna().sum()) #imprime a soma de valores NaN (Not a Number)

#verificar os dados
print(metrics_df.head()metrics_df[['stock_symbol', 'company', 'setor', 'cagr', 'volatility') #imprime as primeiras 5 linhas do dataframe para verificar os dados calculados

#salvar o resultado da segmentação por setor em uma nova tabela
metrics_df.to_csv('segmentacao_setor.csv', index=False) #index=False evita salvar o índice do dataframe no arquivo CSV

print(metrics_df.info)

#Visualização
#gráfico barras da segmentação por setor
def plot_sector_analysis(df):
    setores = df['setor'].unique()
    plt.figure(figsize=(8, 6))

    #média de CAGR e volatilidade por setor
    mean_cagr = df.groupby('setor')['cagr'].mean()
    mean_volatility = df.groupby('setor')['volatility'].mean()

    #gráfico de barras
    bar_width = 0.4
    index = np.arange(len(setores))
    opacity = 0.6

    plt.bar(index, mean_cagr, bar_width, alpha=opacity, color='b', label='CAGR')
    plt.bar(index + bar_width, mean_volatility, bar_width, alpha=opacity, color='g', label='Volatilidade')

    plt.xlabel('Setor')
    plt.ylabel('Média')
    plt.title('Média de CAGR e Volatilidade por Setor')
    plt.xticks(index + bar_width / 2, setores)
    plt.legend()

    plt.tight_layout()
    plt.show()

#chamar a função de plotagem
plot_sector_analysis(metrics_df)

"""**Empresas com Alto CAGR e Alta Volatilidade:**
1.   Tesla, Inc.
2.   NVIDIA Corporation
3.   Apple Inc.
4.   Microsoft Corporation
*   Essas empresas apresentaram forte crescimento nos últimos anos, mas também experimentaram alta oscilação nos preços das ações.
*   São adequadas para investidores com alta tolerância ao risco que buscam retornos potencialmente altos.

**Empresas com Alto CAGR e Baixa Volatilidade:**
1.   Oracle Corporation
2.   Adobe Inc.
*   Essas empresas combinam bom crescimento com relativa estabilidade nos preços das ações.
*   Podem ser atraentes para investidores que buscam um equilíbrio entre retorno e risco.

**Empresas com Baixo CAGR e Alta Volatilidade:**
1.   Meta Platforms, Inc.
2.   Netflix, Inc.
*   Essas empresas apresentaram baixo crescimento e alta oscilação nos preços das ações, indicando potencial incerteza sobre seu futuro.
*   Devem ser analisadas com cautela por investidores, considerando outros fatores além do CAGR e volatilidade.

**Empresas com Baixo CAGR e Baixa Volatilidade:**
1.   Amazon.com, Inc.
2.   Salesforce, Inc.
3.   Cisco Systems, Inc.
4.   Alphabet Inc. (Google)
5.   International Business Machines Corporation (IBM)
6.   Intel Corporation
*   Essas empresas apresentaram crescimento lento e relativa estabilidade nos preços das ações.
*   Podem ser adequadas para investidores conservadores que buscam renda passiva e proteção contra perdas significativas.

# Risco Relativo

Risco Relativo de cada empresa, definido como a razão entre volatilidade e CAGR, para identificar as ações com maior risco (valores altos) ou menor risco (valores baixos) relativo em relação ao seu potencial de crescimento.
"""

#Risco Relativo
import matplotlib.colors as colors

#carregar o DataFrame a partir do arquivo de segmentação por setor para um dataframe chamado df_original
#lê o arquivo CSV contendo os resultados da análise de CAGR e volatilidade realizada anteriormente (segmentacao_setor.csv)
df_original = pd.read_csv('/content/segmentacao_setor.csv')

#cópia do df_original para evitar modificar o dataframe original durante a análise
df_copy = df_original.copy()

#calcular o risco relativo
#adc uma nova coluna risco_relativo ao df_copy, calculando a razão entre volatilidade e CAGR para cada empresa
df_copy['risco_relativo'] = df_copy['volatility'] / df_copy['cagr']

#ordenação por risco relativo em ordem decrescente (do maior para o menor)
df_copy = df_copy.sort_values(by='risco_relativo', ascending=False)

#salvar os resultados do RR em uma nova tabela chamada risco_relativo
df_copy.to_csv('/content/risco_relativo.csv', index=False) #o index=False evita salvar o índice do dataframe no arquivo CSV

#resultados descritos
print(df_copy[['stock_symbol', 'company', 'setor', 'cagr', 'volatility', 'risco_relativo']])

#gráfico de barras horizontal
plt.figure(figsize=(10, 8))

#colormap personalizado
cmap = colors.LinearSegmentedColormap.from_list("", ["green", "white", "red"])
#cmap = colors.LinearSegmentedColormap.from_list("", ["red", "orange", "yellow", "green"])


# Normalizar os valores de risco relativo para aplicar o colormap
norm = plt.Normalize(df_copy['risco_relativo'].min(), df_copy['risco_relativo'].max())
sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)
sm.set_array([])

#aplicar o colormap às barras
bar_colors = cmap(norm(df_copy['risco_relativo'].values))

#criar o gráfico de barras horizontal
bars = plt.barh(df_copy['company'], df_copy['risco_relativo'], color=bar_colors)
plt.xlabel('Risco Relativo') #rótulo do eixo x como "Risco Relativo"
plt.title('Risco Relativo das Empresas') #título do gráfico

#adc valores numéricos ao lado de cada barra
for index, value in enumerate(df_copy['risco_relativo']):
    plt.text(value, index, f'{value:.2f}') #valor formatado com duas casas decimais

#adc barra de cores
plt.colorbar(sm) #função para adicionar uma barra de cores ao gráfico, facilitando a interpretação dos valores de risco relativo

#exibe o gráfico criado
plt.show()

"""A técnica de risco relativo, compara a volatilidade (oscilação do preço da ação) com o retorno (CAGR) para avaliar o risco de cada empresa.

Empresas com alto risco relativo apresentam alta volatilidade em relação ao seu retorno, enquanto empresas com baixo risco relativo apresentam menor oscilação em relação ao seu retorno.

**Empresas com Baixo Risco Relativo:**
*   **ORCL (Oracle):** Risco relativo de 2.61, com bom retorno (CAGR de 11.5%) e volatilidade baixa (30.0%).
*   **NVDA (NVIDIA):** Risco relativo de 2.55, com retorno excelente (CAGR de 20.4%) e volatilidade alta (52.0%).
*   **TSLA (Tesla):** Risco relativo de 1.64, com retorno muito alto (CAGR de 40.1%) e volatilidade altíssima (65.8%).
*   **MSFT (Microsoft):** Risco relativo de 1.35, com bom retorno (CAGR de 23.1%) e volatilidade baixa (31.2%).
*   **AAPL (Apple):** Risco relativo de 1.29, com retorno excelente (CAGR de 26.0%) e volatilidade moderada (33.7%).


**Empresas com Risco Relativo Moderado:**
*   **AMZN (Amazon):** Risco relativo de 8.05, com bom retorno (CAGR de 4.5%) e volatilidade moderada (35.9%).
*   **CSCO (Cisco Systems):** Risco relativo de 5.36, com retorno decente (CAGR de 5.5%) e volatilidade baixa (29.4%).
*   **GOOGL (Alphabet):** Risco relativo de 3.62, com bom retorno (CAGR de 8.7%) e volatilidade baixa (31.5%).
*   **ADBE (Adobe):** Risco relativo de 3.28, com retorno alto (CAGR de 11.4%) e volatilidade moderada (37.6%).


**Empresas com Maior Risco Relativo:**

*   **NFLX (Netflix):** Maior risco relativo (21.54), combinando alto retorno (CAGR de 2.2%) com alta volatilidade (47.7%).
*   **CRM (Salesforce):** Segundo maior risco relativo (11.86), com bom retorno (CAGR de 3.3%) e volatilidade moderada (39.3%).
*   **IBM (International Business Machines):** Terceiro maior risco relativo (10.67), com retorno modesto (CAGR de 2.6%) e volatilidade baixa (27.5%).


**Empresas com Desempenho Negativo:**
*   **INTC (Intel Corporation):** Risco relativo (-4.73) com retorno negativo (CAGR de -0.08%) sugere um declínio no valor das ações nos últimos anos. A volatilidade moderada (0.38) indica que o preço das ações oscilam dentro de uma faixa razoável, mas ainda apresentam risco.

✚ **Setor em transformação:** A indústria de semicondutores enfrenta desafios e incertezas, o que pode afetar o futuro da empresa e pode ser uma explicação para o eclínio no valor das ações da Intel.*

✚ **Recomendações:** Considerar com cautela, devido ao alto risco relativo e ao desempenho recente negativo. Realizar uma análise aprofundada do setor e da empresa antes de tomar decisões de investimento.

*   **META - Meta Platforms, Inc.:** Risco relativo (-8.38) com retorno negativo (CAGR de -0.05%) sugere uma queda significativa no valor das ações nos últimos anos. A volatilidade moderada (0.43) indica que o preço da ação oscila com amplitude considerável, elevando o risco para investimentos.

✚ **Desafios no setor:** A Meta, empresa do Facebook, enfrenta forte concorrência e incertezas regulatórias, impactando negativamente o seu futuro.

✚ **Recomendações:** Altamente desaconselhável para investimentos, devido ao risco extremamente alto e ao desempenho recente negativo. Buscar alternativas com menor risco e perspectivas mais promissoras.

# Análise por Coorte
"""

import pandas as pd
import plotly.graph_objects as go #essa biblioteca oferece mais controle e flexibilidade para a criação de gráficos personalizados

#import do df
df_original = pd.read_csv('/content/stock_prices_clean.csv')

#garantir que a coluna 'date' esteja no formato datetime
df_original['date'] = pd.to_datetime(df_original['date'])

#cópia do df_original para evitar modificar o dataframe original durante a análise
df_copy = df_original.copy()

#criar a nova coluna 'year' a partir de date
df_copy['year'] = df_copy['date'].dt.year

#calcular a média de 'adj_close' por 'company' e 'year'
#agrupa por company (empresa) e year (ano) dentro de cada grupo calcula a média de adj_close e armazena em cohort_analysis
cohort_analysis = df_copy.groupby(['company', 'year'])['adj_close'].mean().unstack()  #a função unstack() transforma o índice multinível (empresa e ano) em colunas separadas

#salvar o resultado em uma nova tabela chamada analise_coorte
cohort_analysis.to_csv('/content/analise_coorte.csv')

#exibir a análise de coorte
print('Análise de Coorte: ')
print(cohort_analysis)

#criar heatmap interativo com Plotly
fig = go.Figure(data=go.Heatmap(
    z=cohort_analysis.values,
    x=cohort_analysis.columns,
    y=cohort_analysis.index,
    colorscale='Spectral',
    colorbar=dict(title='Média de Preço Ajustado de Fechamento')
))

fig.update_layout(
    title='Análise de Coorte dos Preços Ajustados de Fechamento',
    xaxis=dict(title='Ano'),
    yaxis=dict(title='Empresa')
)

fig.show()

"""**Desempenho por Empresa:**

*   **Adobe:** Crescimento significativo ao longo do período, com destaque para o salto entre 2018 e 2019.
*   **Alphabet:** Crescimento sólido, com aumentos constantes e poucas variações.
*   **Amazon:** Crescimento consistente, com aceleração em 2020.
*   **Apple:** Crescimento notável, com aumentos significativos em anos específicos.
*   **Cisco:** Crescimento moderado, com certa estabilidade ao longo do tempo.
*   **Intel:** Crescimento lento, com quedas pontuais em alguns anos.
*   **IBM:** Crescimento estagnado, com aumento expressivo apenas no início do período.
*   **Meta Platforms (Facebook):** Crescimento exponencial, especialmente a partir de 2014.
*   **Microsoft:** Crescimento consistente, com aumentos expressivos a partir de 2018.
*   **NVIDIA:** Crescimento variável, com altos e baixos ao longo do período.
*   **Netflix:** Crescimento exponencial, com aumentos vertiginosos em anos específicos.
*   **Oracle:** Crescimento moderado, com certa estabilidade.
*   **Salesforce:** Crescimento significativo, com aumentos expressivos em determinados anos.
*   **Tesla:** Crescimento inicial lento, seguido por aumentos expressivos a partir de 2019.


---


*OBS: Os dados de 2023 não estão disponíveis para todas as empresas, dificultando a comparação com o ano anterior.*

# Análise de Significância

A análise de significância estatística é um procedimento que verifica a discrepância entre uma hipótese estatística e os dados observados, utilizando uma medida de evidência chamada p-valor.

 **Teste de Shapiro-Wilk:**
- Realizado para cada segmento de crescimento (Alto, Moderado e Baixo) a fim de verificar se a distribuição do CAGR é normal.
- Se p > 0.05, a amostra parece normal (não há evidências para rejeitar a hipótese nula de normalidade).
- Se p <= 0.05, a amostra não parece normal (há evidências para rejeitar a hipótese nula de normalidade).

**Resultados:**

- Alto Crescimento (p-valor = 0.993): A distribuição do CAGR para o segmento de Alto Crescimento parece normal.
- Crescimento Moderado (p-valor = 0.539): A distribuição do CAGR para o segmento de Crescimento Moderado também parece normal.
- Baixo Crescimento (p-valor = 0.076): A distribuição do CAGR para o segmento de Baixo Crescimento pode não ser estritamente normal, mas a evidência contra a normalidade é fraca.

**Teste ANOVA:**
- Realizado para verificar se existem diferenças significativas no CAGR entre os segmentos de crescimento comparando o CAGR entre os 3 segmentos.
- Se p < 0.05, há diferenças significativas entre os segmentos de crescimento (rejeita a hipótese nula de igualdade das médias).
- Se p >= 0.05, não há diferenças significativas entre os segmentos de crescimento (não rejeita a hipótese nula de igualdade das médias).

**Resultados:**

- As empresas de alto crescimento apresentam CAGR médios significativamente maiores do que as empresas de crescimento moderado e baixo.

**O valor de p é comparado com o nível de significância (0.05).**

- Os testes de Shapiro-Wilk sugerem que as distribuições de CAGR para os três segmentos podem ser consideradas aproximadamente normais, com a exceção de uma possível leve anormalidade no segmento de Baixo Crescimento.
- O teste ANOVA confirma que existem diferenças estatisticamente significativas no CAGR entre os segmentos de crescimento.
"""

from scipy.stats import shapiro, f_oneway

#dados do CAGR fornecidos manualmente
cagr_data = {
    'company': ['Apple Inc.', 'Adobe Inc.', 'Amazon.com, Inc.', 'Salesforce, Inc.', 'Cisco Systems, Inc.',
                'Alphabet Inc.', 'IBM Corporation', 'Intel Corporation',
                'Meta Platforms, Inc.', 'Microsoft Corporation', 'Netflix, Inc.', 'NVIDIA Corporation',
                'Oracle Corporation', 'Tesla, Inc.'],
    'cagr': [35.92, 10.76, 2.27, -0.56, 5.73, 13.84, 11.74, -11.16, 1.33, 25.67, 2.13, 44.31, 17.84, 55.99]
}

#criação do df de CAGR
df_cagr = pd.DataFrame(cagr_data)

#função para segmentar o crescimento do CAGR
def segmentar_cagr(cagr):
    if cagr > 20:
        return 'Alto Crescimento'
    elif 10 < cagr <= 20:
        return 'Crescimento Moderado'
    else:
        return 'Baixo Crescimento'

#segmentação ao DataFrame de CAGR
df_cagr['segmento_cagr'] = df_cagr['cagr'].apply(segmentar_cagr)

#gráfico de barras
plt.figure(figsize=(14, 6))
sns.barplot(x='company', y='cagr', hue='segmento_cagr', data=df_cagr, palette='rainbow')
plt.title('CAGR por Empresa e Segmento')
plt.xlabel('Empresa')
plt.ylabel('CAGR (%)')
plt.xticks(rotation=45)
plt.legend(title='Segmento de Crescimento')
plt.show()

#Teste de Normalidade para cada grupo de segmento de crescimento
for segmento in df_cagr['segmento_cagr'].unique():
    segmento_data = df_cagr[df_cagr['segmento_cagr'] == segmento]['cagr']
    stat, p = shapiro(segmento_data)
    print(f'Teste de Shapiro-Wilk para o segmento {segmento}: Estatística={stat:.3f}, p-valor={p:.3f}')
    if p > 0.05:
        print('   Amostra parece normal (não rejeita H0)')
    else:
        print('   Amostra não parece normal (rejeita H0)')

#Teste ANOVA para verificar diferenças entre os segmentos de crescimento
stat, p = f_oneway(
    df_cagr[df_cagr['segmento_cagr'] == 'Alto Crescimento']['cagr'],
    df_cagr[df_cagr['segmento_cagr'] == 'Crescimento Moderado']['cagr'],
    df_cagr[df_cagr['segmento_cagr'] == 'Baixo Crescimento']['cagr']
)
print('\nTeste ANOVA para CAGR entre os segmentos de crescimento:')
print(f'Estatística F={stat:.3f}, p-valor={p:.3f}')
if p < 0.05:
    print('   Há diferenças significativas entre os segmentos de crescimento (rejeita H0)')
else:
    print('   Não há diferenças significativas entre os segmentos de crescimento (não rejeita H0)')

"""# Hipóteses

Hipóteses sobre os dados das big techs

Para modelar a relação entre o preço das ações e outras variáveis relevantes e prever os preços ajustados de fechamento (adj_close).

## Hipótese 1

**As ações das big techs com maior crescimento anual composto (CAGR) apresentam menor volatilidade nos retornos diários.**

*   **Refutada:** A relação é oposta ao esperado (maior crescimento está associado a maior volatilidade).
"""

import numpy as np
from scipy.stats import pearsonr

#coluna 'date' do tipo datetime
df['date'] = pd.to_datetime(df['date'])

#1-calcular o CAGR para cada empresa:
def calculate_cagr(start_value, end_value, periods):
    return (end_value/start_value)**(1/periods) - 1

cagr_dict = {}
for company in df['company'].unique():
    company_data = df[df['company'] == company]
    start_value = company_data.iloc[0]['adj_close']
    end_value = company_data.iloc[-1]['adj_close']
    periods = (company_data['date'].iloc[-1] - company_data['date'].iloc[0]).days / 365
    cagr_dict[company] = calculate_cagr(start_value, end_value, periods)

cagr_df = pd.DataFrame(list(cagr_dict.items()), columns=['company', 'CAGR'])


#2-calcular a volatilidade dos retornos diários para cada empresa:
volatility_dict = {}
for company in df['company'].unique():
    company_data = df[df['company'] == company]
    daily_returns = company_data['adj_close'].pct_change().dropna()
    volatility_dict[company] = daily_returns.std()

volatility_df = pd.DataFrame(list(volatility_dict.items()), columns=['company', 'Volatility'])


#3-analisar a correlação entre CAGR e volatilidade dos retornos diários:
analysis_df = pd.merge(cagr_df, volatility_df, on='company')
correlation = analysis_df['CAGR'].corr(analysis_df['Volatility'])
print(f"Correlação entre CAGR e Volatilidade: {correlation}")

#from scipy.stats import pearsonr
corr, p_value = pearsonr(analysis_df['CAGR'], analysis_df['Volatility'])
print(f"Teste de correlação de Pearson: Correlação = {corr}, Valor p = {p_value}")

"""Visualizar a relação entre CAGR e volatilidade usando um gráfico de dispersão para ilustrar a correlação positiva entre o crescimento das ações e sua volatilidade."""

#gráfico de dispersão
plt.figure(figsize=(10, 6))
sns.scatterplot(x=analysis_df['CAGR'], y=analysis_df['Volatility'])

#adc títulos e rótulos
plt.title('Relação entre CAGR e Volatilidade das Ações das Big Techs')
plt.xlabel('CAGR')
plt.ylabel('Volatilidade dos Retornos Diários')
plt.show()

"""**Resultados**

*   Correlação entre CAGR e Volatilidade: 0.8162492756984856
*   Teste de correlação de Pearson:
1.   Correlação = 0.8162492756984858
2.   Valor p = 0.00036933838682115354


**Análise**

*   Correlação Forte e Positiva: A correlação de 0.8162 indica uma relação forte e positiva entre CAGR e volatilidade.
*   Significância Estatística: O valor p é significativamente menor que 0.05, indicando que a correlação observada é estatisticamente significativa.

**Conclusão**

*   **Hipótese Refutada:** Embora exista uma correlação forte e significativa, a relação é positiva, o que significa que as ações com maior crescimento (CAGR) tendem a ter maior volatilidade, não menor. Portanto, a hipótese de que as ações com maior crescimento apresentam menor volatilidade é refutada. Na verdade, elas tendem a ser mais voláteis.

## Hipótese 2

**As empresas de tecnologia com maior volume de negociação apresentam menor volatilidade nos retornos diários.**

*   **Refutada:** Não há relação significativa entre volume de negociação e volatilidade.
"""

#calcular o volume médio de negociação diário para cada empresa
avg_volume = df.groupby('company')['volume'].mean().reset_index()
avg_volume.columns = ['company', 'avg_volume']

#unir volatilidade e volume médio
volatility_volume_df = pd.merge(analysis_df, avg_volume, on='company')

#correlação entre volume médio de negociação e volatilidade
correlation = volatility_volume_df['avg_volume'].corr(volatility_volume_df['Volatility'])
pearson_corr = pearsonr(volatility_volume_df['avg_volume'], volatility_volume_df['Volatility'])

#resultados
print(f"Correlação entre Volume Médio de Negociação e Volatilidade: {correlation}")
print(f"Teste de correlação de Pearson: Correlação = {pearson_corr[0]}, Valor p = {pearson_corr[1]}")

"""Visualizar a relação entre o volume médio de negociação e a volatilidade dos retornos diários com um gráfico de dispersão para confirmar a ausência de uma relação clara entre as duas variáveis."""

#gráfico de dispersão
plt.figure(figsize=(10, 6))
sns.scatterplot(x=volatility_volume_df['avg_volume'], y=volatility_volume_df['Volatility'])

#adc títulos e rótulos
plt.title('Relação entre Volume Médio de Negociação e Volatilidade das Ações das Big Techs')
plt.xlabel('Volume Médio de Negociação')
plt.ylabel('Volatilidade dos Retornos Diários')
plt.show()

"""**Resultados**

*   Correlação entre Volume Médio de Negociação e Volatilidade: 0.017926989088759625
*   Teste de correlação de Pearson:
1.   Correlação = 0.0179269890887596
2.   Valor p = 0.9514970637555173


**Análise**

*   Correlação Quase Inexistente: A correlação de 0.0179 é praticamente zero, indicando nenhuma relação significativa entre o volume médio de negociação e a volatilidade.
*   Significância Estatística: O valor p é muito alto, indicando que a correlação observada não é estatisticamente significativa.

**Conclusão**

*   **Hipótese Refutada:** A correlação muito próxima de zero e o valor p alto indicam que não há uma relação significativa entre o volume médio de negociação e a volatilidade dos retornos diários.
Não há evidências para sugerir que o volume médio de negociação tem qualquer efeito sobre a volatilidade dos retornos diários. Portanto, a hipótese não é suportada pelos dados.

# Regressão Linear

### Regressão Linear Simples

Para prever os preços ajustados de fechamento (adj_close) com base em apenas 1 variável independente: preços de abertura (open)
"""

#import das bibliotecas necessárias

from sklearn.model_selection import train_test_split #para dividir os dados em conjuntos de treinamento e teste
from sklearn.linear_model import LinearRegression #cria uma instância e treina o modelo com os dados de treinamento
from sklearn.metrics import mean_squared_error, r2_score

#preparação dos dados
X = df[['open']]  #variável independente
y = df['adj_close']  #variável dependente

#dividir os dados em conjuntos de treinamento e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#treinar o modelo de regressão
model = LinearRegression()
model.fit(X_train, y_train)

#fazer previsões utilizando o modelo treinado para prever os valores do conjunto de teste
y_pred = model.predict(X_test)

#avaliação do modelo
mse = mean_squared_error(y_test, y_pred) #erro quadrático médio (MSE)
r2 = r2_score(y_test, y_pred)  #coeficiente de determinação (R^2) para avaliar a performance do modelo

print(f'Mean Squared Error: {mse}')
print(f'R^2 Score: {r2}')

#visualizar os resultados utilizando matplotlib para criar gráfico que compara os dados reais com as previsões
plt.figure(figsize=(10, 6))
plt.scatter(X_test, y_test, color='blue', label='Dados Reais')
plt.plot(X_test, y_pred, color='red', linewidth=2, label='Previsões')
plt.title('Regressão Linear Simples - Previsão de Preços Ajustados de Fechamento')
plt.xlabel('Preço de Abertura')
plt.ylabel('Preço Ajustado de Fechamento')
plt.legend()
plt.show()

"""Os resultados indicam que o modelo de regressão linear está se ajustando bem aos dados:


*   **Mean Squared Error (MSE):** O valor de 113.36788008864822 representa a média dos quadrados dos erros entre os valores preditos e os valores reais.

Um MSE mais baixo é desejável, indicando que os valores preditos estão próximos dos valores reais.

*   **R^2 Score:** O valor de 0.9893708158396498 está próximo de 1, o que significa que o modelo está explicando 98.94% da variabilidade dos dados em torno da média.

Isso indica um excelente ajuste do modelo.

### Regressão Linear Múltipla

Para prever os preços ajustados de fechamento (adj_close) com mais variáveis independentes que visam melhorar ainda mais a precisão do modelo: preços de abertura, preço em alta, preço em baixa e volume das negociações (open, high, low, volume)
"""

#preparação dos dados com mais variáveis independentes
X = df[['open', 'high', 'low', 'volume']]  #variáveis adicionais
y = df['adj_close']

#dividir os dados em conjuntos de treinamento e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#treinar o modelo de regressão
model = LinearRegression()
model.fit(X_train, y_train)

#fazer previsões utilizando o modelo treinado para prever os valores do conjunto de teste
y_pred = model.predict(X_test)

#avaliação do modelo
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f'Mean Squared Error: {mse}')
print(f'R^2 Score: {r2}')

#visualizar os resultados utilizando matplotlib para criar gráfico que compara os dados reais com as previsões
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, color='blue', label='Valores Preditos vs. Reais')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linewidth=2, label='Linha de Perfeição')
plt.title('Regressão Linear Múltipla - Previsão de Preços Ajustados de Fechamento')
plt.xlabel('Preço Ajustado de Fechamento Real')
plt.ylabel('Preço Ajustado de Fechamento Predito')
plt.legend()
plt.show()

"""*   **Análise Residual**

Para verificar a distribuição dos resíduos (a diferença entre os valores reais e os valores preditos) e garantir que não haja padrões não capturados pelo modelo.

**Os resíduos foram distribuídos aleatoriamente para garantir a qualidade do modelo.**
"""

#calcular os resíduos
residuals = y_test - y_pred

#visualizar os resíduos
plt.figure(figsize=(10, 6))
sns.histplot(residuals, bins=50, kde=True)
plt.title('Distribuição dos Resíduos')
plt.xlabel('Resíduo')
plt.ylabel('Frequência')
plt.show()

# Gráfico de Resíduos vs. Valores Preditos
plt.figure(figsize=(10, 6))
plt.scatter(y_pred, residuals)
plt.axhline(y=0, color='red', linestyle='--')
plt.title('Gráfico de Resíduos vs. Valores Preditos')
plt.xlabel('Valores Preditos')
plt.ylabel('Resíduos')
plt.show()

"""*   **Validação Cruzada**

Verifica a estabilidade e a generalização para garantir que o modelo não esteja superajustado aos dados de treinamento.

**A validação cruzada confirma a estabilidade do modelo, assegurando que ele não esteja superajustado aos dados de treinamento.**
"""

from sklearn.model_selection import cross_val_score

# Realizar validação cruzada
cv_scores = cross_val_score(model, X, y, cv=5, scoring='r2')
print(f'Scores de Validação Cruzada: {cv_scores}')
print(f'Média dos Scores de Validação Cruzada: {cv_scores.mean()}')

"""Os resultados da validação cruzada mostram a performance do modelo em diferentes subconjuntos dos dados, ajudando a avaliar sua estabilidade e generalização.

A maioria dos scores estão acima de 0.99, indicando um excelente ajuste na maioria dos subconjuntos de dados.
No entanto, um dos scores é significativamente menor, 0.65734819, indicando que o modelo teve um desempenho pior em um dos subconjuntos de dados.

*   **Excelente Desempenho Geral:**

A média dos scores de validação cruzada é alta (0.928), indicando que o modelo, em geral, tem um bom desempenho na previsão dos preços ajustados de fechamento das ações.

A maioria dos scores individuais estão acima de 0.99, o que mostra que o modelo é altamente preciso na maioria dos casos.

*   **Inconsistência em Alguns Subconjuntos:**

O score significativamente menor (0.65734819) sugere que o modelo pode estar superajustado aos dados de treinamento e pode não generalizar bem para todos os subconjuntos de dados.

Essa inconsistência pode ser devida a variabilidade nos dados e/ou falta de algumas variáveis explicativas.

*   Analisar o Subconjunto com Desempenho Baixo
*   Ajustes no Modelo
"""

#Análise do Subconjunto com Baixo Desempenho:
from sklearn.model_selection import KFold

kf = KFold(n_splits=5, random_state=42, shuffle=True)
for train_index, test_index in kf.split(X):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    score = r2_score(y_test, y_pred)
    if score < 0.7:
        # Analisar este subconjunto
        print("Subconjunto com baixo desempenho:")
        print(X_test)
        print(y_test)

#Aplicar Regularização:
from sklearn.linear_model import Ridge

ridge_model = Ridge(alpha=1.0)
scores = cross_val_score(ridge_model, X, y, cv=5, scoring='r2')
print(f'Scores de Validação Cruzada com Ridge: {scores}')
print(f'Média dos Scores de Validação Cruzada com Ridge: {scores.mean()}')

"""**Os resultados com a Ridge Regression são praticamente idênticos aos obtidos anteriormente com a regressão linear simples, sugerindo que a regularização não teve um impacto significativo na melhoria do desempenho do modelo.**"""